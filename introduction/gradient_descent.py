#Ref:[Machine Learning Tutorial Python - 4: Gradient Descent and Cost Function](https://www.youtube.com/watch?v=vsWrXfO3wWw)


import numpy as np


def gradient_descent(x,y):

    weight = 0
    bias = 0

    n = len(x)

    iterations = 10
    learning_rate = 0.1

    for i in range(iterations):

        y_pred = weight * x + bias  # regression
        loss = 1/n * sum([loss_value ** 2 for loss_value in (y - y_pred)])

        derivative_weight = -(2/n) * sum(x*(y - y_pred))
        derivative_bias = -(2/n) * sum(y - y_pred)

        weight = weight - learning_rate * derivative_weight
        bias = bias - learning_rate * derivative_bias

        print("w: {}, b: {}, dw: {}, db: {}, loss: {}, iter: {}".format(weight, bias, derivative_weight, derivative_bias, loss, i))


x = np.array([1, 2, 3, 4, 5])
y = np.array([5, 7, 9, 11, 13])

gradient_descent(x,y)

'''
iterations = 10
learning_rate = 1

weight 62.0, bias 18.0, loss 89.0, iteration 0
weight -1348.0, bias -372.0, loss 45225.0, iteration 1
weight 30602.0, bias 8478.0, loss 23225625.0, iteration 2
weight -693448.0, bias -192072.0, loss 11927795625.0, iteration 3
weight 15714902.0, bias 4352778.0, loss 6125661185625.0, iteration 4
weight -356129548.0, bias -98642172.0, loss 3145906095395625.0, iteration 5
weight 8070573602.0, bias 2235419478.0, loss 1.6156174592665856e+18, iteration 6
weight -182894562448.0, bias -50658861072.0, loss 8.29719545191561e+20, iteration 7
weight 4144738977902.0, bias 1148026235778.0, loss 4.261123323001274e+23, iteration 8
weight -93927675950548.0, bias -26016460103172.0, loss 2.1883505190459738e+26, iteration 9


iterations = 10
learning_rate = 0.1

weight 6.2, bias 1.8, loss 89.0, iteration 0
weight -2.320000000000001, bias -0.4800000000000002, loss 165.24, iteration 1
weight 9.272000000000002, bias 2.8080000000000007, loss 307.59840000000014, iteration 2
weight -6.611200000000007, bias -1.5168000000000017, loss 573.3613440000004, iteration 3
weight 15.043520000000012, bias 4.5532800000000035, loss 1069.4523110400019, iteration 4
weight -14.584192000000016, bias -3.5834880000000053, loss 1995.4416651264037, iteration 5
weight 25.851123200000032, bias 7.6837248000000065, loss 3723.822955597831, iteration 6
weight -29.431582720000037, bias -7.563694080000013, loss 6949.846718357686, iteration 7
weight 46.056115712000064, bias 13.407994368000011, loss 12971.181693567853, iteration 8
weight -57.11213547520009, bias -15.10727393280003, loss 24209.895819923568, iteration 9


iterations = 10
learning_rate = 0.01

weight 0.62, bias 0.18, loss 89.0, iteration 0
weight 1.0928, bias 0.3192, loss 52.25039999999999, iteration 1
weight 1.453232, bias 0.42724799999999996, loss 30.831949440000002, iteration 2
weight 1.7278860800000002, bias 0.5115091199999999, loss 18.347751350784, iteration 3
weight 1.9370605952000002, bias 0.5776057727999999, loss 11.070010749324897, iteration 4
weight 2.096250917888, bias 0.6298300216319999, loss 6.826353152519786, iteration 5
weight 2.2172859146547204, bias 0.6714583661260799, loss 4.350826141683065, iteration 6
weight 2.309195511463117, bias 0.7049920439242751, loss 2.9056952040975976, iteration 7
weight 2.3788729763057748, bias 0.7323404723580026, loss 2.0610450731046615, iteration 8
weight 2.431580493177024, bias 0.7549612843324961, loss 1.5663423003130599, iteration 9

iterations = 10
learning_rate = 0.001

weight 0.062, bias 0.018000000000000002, loss 89.0, iteration 0
weight 0.122528, bias 0.035592000000000006, loss 84.881304, iteration 1
weight 0.181618832, bias 0.052785648000000004, loss 80.955185108544, iteration 2
weight 0.239306503808, bias 0.069590363712, loss 77.21263768455901, iteration 3
weight 0.29562421854195203, bias 0.086015343961728, loss 73.64507722605434, iteration 4
weight 0.35060439367025875, bias 0.10206956796255283, loss 70.2443206760065, iteration 5
weight 0.40427867960173774, bias 0.11776180246460617, loss 67.00256764921804, iteration 6
weight 0.4566779778357119, bias 0.13310060678206653, loss 63.912382537082294, iteration 7
weight 0.5078324586826338, bias 0.14809433770148814, loss 60.966677449199324, iteration 8
weight 0.5577715785654069, bias 0.16275115427398937, loss 58.15869595270883, iteration 9


iterations = 50
learning_rate = 0.01

weight 0.62, bias 0.18, loss 89.0, iteration 0
weight 1.0928, bias 0.3192, loss 52.25039999999999, iteration 1
weight 1.453232, bias 0.42724799999999996, loss 30.831949440000002, iteration 2
weight 1.7278860800000002, bias 0.5115091199999999, loss 18.347751350784, iteration 3
weight 1.9370605952000002, bias 0.5776057727999999, loss 11.070010749324897, iteration 4
weight 2.096250917888, bias 0.6298300216319999, loss 6.826353152519786, iteration 5
weight 2.2172859146547204, bias 0.6714583661260799, loss 4.350826141683065, iteration 6
weight 2.309195511463117, bias 0.7049920439242751, loss 2.9056952040975976, iteration 7
weight 2.3788729763057748, bias 0.7323404723580026, loss 2.0610450731046615, iteration 8
weight 2.431580493177024, bias 0.7549612843324961, loss 1.5663423003130599, iteration 9
weight 2.471335107618129, bias 0.7739672290552247, loss 1.2755882811903883, iteration 10
weight 2.501203350198827, bias 0.7902077780170325, loss 1.1037021437989751, iteration 11
weight 2.523526146474063, bias 0.8043314214447622, loss 1.0011028556851886, iteration 12
weight 2.5400905089630834, bias 0.8168332242274232, loss 0.9388969341253388, iteration 13
weight 2.55226060353756, bias 0.8280911292050898, loss 0.9002465769594267, iteration 14
weight 2.561077803006991, bias 0.8383936704087344, loss 0.8753394365713272, iteration 15
weight 2.567337066120929, bias 0.8479611288201402, loss 0.8584572531892551, iteration 16
weight 2.5716452438451163, bias 0.8569616822764816, loss 0.8462675994905574, iteration 17
weight 2.574465589262602, bias 0.865523734000245, loss 0.8368283939224842, iteration 18
weight 2.5761517355848147, bias 0.8737453239644839, loss 0.8290077803438447, iteration 19
weight 2.5769736343182865, bias 0.8817013133501054, loss 0.8221460625950908, iteration 20
weight 2.577137355967257, bias 0.8894488690240061, loss 0.8158586966973802, iteration 21
weight 2.57680020551302, bias 0.8970316502854906, loss 0.809921484454552, iteration 22
weight 2.576082261283026, bias 0.9044830049489996, loss 0.8042036699632176, iteration 23
weight 2.57507518350382, bias 0.911828409173038, loss 0.7986289515662348, iteration 24
weight 2.5738489385825973, bias 0.9190873299793481, loss 0.7931527615446066, iteration 25
weight 2.572456932295665, bias 0.9262746470648052, loss 0.7877490258420092, iteration 26
weight 2.5709399283667302, bias 0.9334017381857692, loss 0.7824024482830367, iteration 27
weight 2.5693290398349036, bias 0.94047730772005, loss 0.7771040141929204, iteration 28
weight 2.567648012608022, bias 0.9475080191755548, loss 0.7718483701250819, iteration 29
weight 2.5659149686837237, bias 0.9544989780355624, loss 0.7666322968914703, iteration 30
weight 2.564143736891171, bias 0.9614541003538277, loss 0.7614538197156816, iteration 31
weight 2.5623448687538835, bias 0.9683763941332809, loss 0.7563116896697106, iteration 32
weight 2.5605264139800323, bias 0.9752681741253822, loss 0.751205081476467, iteration 33
weight 2.5586945124569023, bias 0.9821312258040726, loss 0.7461334173995673, iteration 34
weight 2.5568538461681394, bias 0.988966930540577, loss 0.741096264610584, iteration 35
weight 2.555007984178714, bias 0.9957763611596772, loss 0.7360932753753443, iteration 36
weight 2.5531596459898163, bias 1.0025603548857607, loss 0.7311241521930939, iteration 37
weight 2.551310902578911, bias 1.0093195690286565, loss 0.7261886274769644, iteration 38
weight 2.5494633298698313, bias 1.0160545234933487, loss 0.7212864517084411, iteration 39
weight 2.5476181258888673, bias 1.0227656332312918, loss 0.716417386530074, iteration 40
weight 2.545776200199439, bias 1.029453233013334, loss 0.7115812007159951, iteration 41
weight 2.5439382421747623, bias 1.036117596341101, loss 0.7067776678194898, iteration 42
weight 2.5421047731158484, bias 1.0427589498837933, loss 0.7020065647979179, iteration 43
weight 2.540276186037334, bias 1.0493774844991666, loss 0.6972676712072055, iteration 44
weight 2.5384527760391706, bias 1.0559733636469433, loss 0.6925607687282819, iteration 45
weight 2.5366347634917363, bias 1.0625467298116542, loss 0.6878856408869934, iteration 46
weight 2.534822311734855, bias 1.0690977094059169, loss 0.6832420728867806, iteration 47
weight 2.5330155405888317, bias 1.0756264165137073, loss 0.67862985150711, iteration 48
weight 2.5312145366684664, bias 1.0821329557481032, loss 0.6740487650402432, iteration 49


iterations = 100
learning_rate = 0.01
...
...
weight 2.4623482709187696, bias 1.3307760394530594, loss 0.5106037458571004, iteration 90
weight 2.4607850889494567, bias 1.336419622408872, loss 0.5071569186909993, iteration 91
weight 2.459227192036044, bias 1.3420441246237271, loss 0.5037333593085942, iteration 92
weight 2.4576745623106904, bias 1.34764961060909, loss 0.5003329106408669, iteration 93
weight 2.456127181965793, bias 1.3532361446582668, loss 0.49695541667909215, iteration 94
weight 2.4545850332538226, bias 1.358803790847154, loss 0.4936007224676837, iteration 95
weight 2.4530480984871526, bias 1.3643526130349815, loss 0.4902686740970767, iteration 96
weight 2.45151636003788, bias 1.3698826748650528, loss 0.48695911869667574, iteration 97
weight 2.4499898003376432, bias 1.375394039765479, loss 0.48367190442783614, iteration 98
weight 2.4484684018774328, bias 1.3808867709499106, loss 0.4804068804768991, iteration 99

..
..
weight 2.021717903040843, bias 2.9215914789692374, loss 0.0011266299717833247, iteration 993
weight 2.0216444756337033, bias 2.921856575207402, loss 0.0011190246637055488, iteration 994
weight 2.0215712964818446, bias 2.922120775165232, loss 0.0011114706952090794, iteration 995
weight 2.021498364745925, bias 2.922384081873017, loss 0.0011039677197263124, iteration 996
weight 2.0214256795894405, bias 2.922646498350801, loss 0.001096515393029289, iteration 997
weight 2.0213532401787155, bias 2.9229080276084183, loss 0.0010891133732137189, iteration 998
weight 2.021281045682893, bias 2.923168672645527, loss 0.0010817613206833153, iteration 999
'''

